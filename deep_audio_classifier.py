# -*- coding: utf-8 -*-
"""Deep_Audio_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1psvfi57h3gwP1Pkvg-_tDO3ITq8O136T
"""

import tensorflow as tf
import os
import matplotlib.pyplot as plt
import tensorflow_io as tfio

NOT_CAPUCHIN_FILE =os.path.join('data', 'Parsed_Not_Capuchinbird_Clips', 'afternoon-birds-song-in-forest-0.wav')
CAPUCHIN_FILE = os.path.join('data', 'Parsed_Capuchinbird_Clips', 'XC3776-0.wav')

#data loading function

def load_wav_16k_mono(filename):
    file_contents = tf.io.read_file(filename)
    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)
    wav = tf.squeeze(wav, axis = -1)
    sample_rate = tf.cast(sample_rate, dtype = tf.int64)
    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)

    return wav

wave = load_wav_16k_mono(CAPUCHIN_FILE)
nwave = load_wav_16k_mono(NOT_CAPUCHIN_FILE)

plt.plot(wave)
plt.plot(nwave)

plt.show()

POS = os.path.join('data', 'Parsed_Capuchinbird_Clips')
NEG = os.path.join('data', 'Parsed_Not_Capuchinbird_Clips')

pos = tf.data.Dataset.list_files(POS+'/*.wav')
neg = tf.data.Dataset.list_files(NEG+'/*.wav')

pos.as_numpy_iterator().next()

positives = tf.data.Dataset.zip((pos, tf.data.Dataset.from_tensor_slices(tf.ones(len(pos)))))
negatives = tf.data.Dataset.zip((neg, tf.data.Dataset.from_tensor_slices(tf.zeros(len(neg)))))

data = positives.concatenate(negatives)

data.shuffle(12000).as_numpy_iterator().next()

lengths = []

for file in os.listdir(os.path.join('data', 'Parsed_Capuchinbird_Clips')):
    tensor_wave = load_wav_16k_mono(os.path.join('data', 'Parsed_Capuchinbird_Clips', file))

    lengths.append(len(tensor_wave))

lengths

#it is important to know the length of the sound you wanna predict on

tf.reduce_mean(lengths)

tf.reduce_min(lengths)

tf.reduce_max(lengths)

#so let's grab around 48000

#now converting our data into the spectrogram

def preprocess(filename, label):
    wav = load_wav_16k_mono(filename)

    wav = wav[:48000]

    zero_padding = tf.zeros([48000] - tf.shape(wav), dtype = tf.float32) #gives us the number of zeroes required to pad the file

    wav = tf.concat([zero_padding, wav],0) #concatenating zeroes and our wav file

    spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)
    spectrogram = tf.abs(spectrogram)  #ocnverting it into the absolute set of values
    spectrogram = tf.expand_dims(spectrogram, axis=2)

    return spectrogram, label

wav = CAPUCHIN_FILE
wav = load_wav_16k_mono(CAPUCHIN_FILE)

wav

wav = wav[:48000]
zero_padding = tf.zeros([48000] - tf.shape(wav), dtype = tf.float32) #gives us the number of zeroes required to pad the file
wav = tf.concat([zero_padding, wav],0) #concatenating zeroes and our wav file

wav

spectrogram = tf.signal.stft(wav, frame_length=320, frame_step=32)

spectrogram

spectrogram = tf.abs(spectrogram)
spectrogram         #all the negative values are gone

#for the sake of neural network, to make it in the format that can be fed to the neural network, let's expand the dims
spectrogram = tf.expand_dims(spectrogram, axis = 2)
spectrogram    #this is the format that we will feed our spectrogram in, to the neural network

filepath, label = positives.shuffle(10000).as_numpy_iterator().next()
spectrogram, label = preprocess(filepath, label)

spectrogram

plt.figure(figsize = (30, 30))
plt.imshow(tf.transpose(spectrogram)[0])
plt.show()

data = data.map(preprocess)
data = data.cache()
data = data.shuffle(buffer_size=1000)
data = data.batch(8)
data = data.prefetch(4)

len(data)

len(data) * .7

train = data.take(72)
test = data.skip(72).take(30)

samples, labels = train.as_numpy_iterator().next()

samples.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D, Dropout, Activation

model = Sequential()
model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu', input_shape = (1491, 257, 1)))
model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))
model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))
model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))
model.add(Conv2D(32, (3, 3), padding = 'same', activation = 'relu'))
model.add(MaxPooling2D(pool_size = (2,2)))

model.add(MaxPooling2D(pool_size = (2,2)))

model.add(Flatten())

model.add(Dense(256, activation = 'relu'))
model.add(Dropout(0.5))

model.add(Dense(256, activation = 'relu'))
model.add(Dropout(0.5))

model.add(Dense(1))
model.add(Activation('sigmoid'))

model.compile('Adam', loss = 'BinaryCrossentropy', metrics = [tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])

model.summary()

hist = model.fit(train, epochs = 4, validation_data=test)

x_test, y_test = test.as_numpy_iterator().next()

yhat = model.predict(x_test)

yhat

yhat = [1 if prediction > 0.5 else 0 for prediction in yhat]

yhat

def load_mp3_16k_mono(filename):

  res = tfio.audio.AudioIOTensor(filename)

  tensor = res.to_tensor()
  tensor = tf.math.reduce_sum(tensor, axis = 1)/2

  sample_rate = res.rate

  sample_rate = tf.cast(sample_rate, tf.int64)
  mp3 = tfio.audio.resample(tensor, rate_in = sample_rate, rate_out=16000)

  return mp3

mp3 = os.path.join('data', 'Forest Recordings', 'recording_00.mp3')

tensor = load_mp3_16k_mono(mp3)

tensor

#now we will convert this big file into a number of audio slices so that we can make multiple predicions on a single audio clip

audio_slices = tf.keras.utils.timeseries_dataset_from_array(tensor, tensor, sequence_length=48000, sequence_stride=48000, batch_size=1) #stride and sequence length same to avoid overlapping

samples, index = audio_slices.as_numpy_iterator().next()

samples.shape

len(audio_slices)

index

def preprocess_mp3(sample, index):
  sample = sample[0]

  zero_padding = tf.zeros([48000] - tf.shape(sample), dtype=tf.float32)
  mp3 = tf.concat([zero_padding, sample], 0)

  spectrogram = tf.signal.stft(mp3, frame_length = 320, frame_step = 32)
  spectrogram = tf.abs(spectrogram)
  spectrogram = tf.expand_dims(spectrogram, axis = 2)

  return spectrogram

audio_slices = tf.keras.utils.timeseries_dataset_from_array(tensor, tensor, sequence_length=48000, sequence_stride=48000, batch_size=1)
audio_slices = audio_slices.map(preprocess_mp3)
audio_slices = audio_slices.batch(64)

yhat = model.predict(audio_slices)
yhat = [1 if prediction > 0.99 else 0 for prediction in yhat]
yhat

tf.math.reduce_sum(yhat)

from itertools import groupby
yhat = [key for key, group in groupby(yhat)]
tf.math.reduce_sum(yhat)

yhat

#making predictions on all the files

results = {}

for file in os.listdir(os.path.join('data', 'Forest Recordings')):
  filename = os.path.join('data', 'Forest Recordings', file)
  mp3 = load_mp3_16k_mono(filename)

  audioslices = tf.keras.utils.timeseries_dataset_from_array(mp3, mp3, sequence_length=48000, sequence_stride = 48000, batch_size = 1)
  audioslices = audioslices.map(preprocess_mp3)
  audioslices = audioslices.batch(64)

  yhat = model.predict(audioslices)

  results[file] = yhat

results

class_preds = {}
for file, logits in results.items():
  class_preds[file] = [1 if prediction > 0.99 else 0 for prediction in logits]

class_preds

post_processed = {}

for file, scores in class_preds.items():
    post_processed[file] = tf.math.reduce_sum([key for key, group in groupby(scores)]).numpy()

post_processed

import csv

with open('results.csv', 'w', newline='') as f:
    writer = csv.writer(f, delimiter=',')
    writer.writerow(['recording', 'capuchin_calls'])

    for key, value in post_processed.items():
        writer.writerow([key, value])

